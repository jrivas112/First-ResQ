@echo off
REM Production deployment script for QHelper AI

echo ğŸš€ Deploying QHelper AI in Production Mode...

REM Stop any running development containers
echo ğŸ“¦ Stopping existing containers...
docker compose down

REM Remove development volumes and build cache
echo ğŸ§¹ Cleaning up development artifacts...
docker system prune -f
docker volume prune -f

REM Build production images
echo ğŸ—ï¸ Building production images...
docker compose build --no-cache

REM Start production services
echo ğŸŒŸ Starting production services...
docker compose up -d

REM Wait for services to start
echo â³ Waiting for services to initialize...
timeout /t 30 /nobreak > nul

REM Download optimal model for production
echo ğŸ¤– Setting up AI model...
docker exec ollama ollama pull qwen2:1.5b

REM Check service status
echo ğŸ” Checking service status...
docker compose ps

echo.
echo âœ… Production deployment complete!
echo.
echo ğŸŒ Frontend (Nginx): http://localhost
echo ğŸ”§ Ollama WebUI: http://localhost:8080
echo ğŸ“Š Backend API (internal): http://backend:8000
echo.
echo ğŸ“ Note: Backend is now only accessible through Nginx proxy
echo ğŸ”’ Production optimizations enabled: compression, caching, security headers

pause
