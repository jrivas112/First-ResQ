# First Res-Q - AI-Powered First Aid Assistant

A comprehensive first aid chatbot application with local AI capabilities, built with FastAPI backend, HTML/CSS/JS frontend, and containerized AI services.

## ðŸš€ Features

- **Interactive Chat Interface** - Ask first aid questions and get immediate responses
- **Dual AI System** - Toggle between Local AI (CSV-based RAG + Ollama) and External AI (AnythingLLM)
- **Enhanced Local AI** - Combines CSV knowledge base with Ollama LLM for intelligent responses
- **Smart Response Methods**:
  - ðŸ¤– **AI + Knowledge Base** - Best responses using both RAG and LLM reasoning
  - ðŸ¤– **AI Reasoning** - Ollama-powered responses for general questions
  - ðŸ“š **Knowledge Base Only** - Fast CSV lookup when LLM unavailable
  - ðŸ†˜ **General Advice** - Safety fallback for unmatched queries
- **Web UI for AI Models** - Manage local models via Ollama Web UI
- **Containerized Architecture** - Complete Docker setup for easy deployment
- **Cross-Platform** - Runs on Windows, macOS, and Linux
- **Health Monitoring** - Built-in health checks and system status

## ðŸ“‹ Prerequisites

Before you begin, ensure you have the following installed:

- **Docker Desktop** (latest version)
- **Docker Compose** (usually included with Docker Desktop)
- **Git** (for cloning the repository)

### System Requirements
- **RAM**: Minimum 8GB (16GB recommended for larger models)
- **Storage**: At least 10GB free space for models and containers
- **CPU**: Multi-core processor (GPU optional but not required)

## ðŸ› ï¸ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/jrivas112/QHelper-AI.git
cd QHelper-AI
```

### 2. Environment Configuration

Create a `.env` file in the `backend` directory:

```bash
# Navigate to backend directory
cd backend

# Create .env file (Windows)
echo ANYTHINGLLM_API_KEY=your_api_key_here > .env
echo ANYTHINGLLM_WORKSPACE_ID=qhelper >> .env

# Create .env file (macOS/Linux)
cat > .env << EOF
ANYTHINGLLM_API_KEY=your_api_key_here
ANYTHINGLLM_WORKSPACE_ID=qhelper
EOF
```

**Note**: Replace `your_api_key_here` with your actual AnythingLLM API key, or leave it as is if you plan to use only local AI models.

### 3. Build and Start Services

From the root directory:

```bash
# Build and start all services
docker compose up --build

# Or run in background (detached mode)
docker compose up -d --build
```

**First startup will take several minutes** as Docker downloads all the necessary images.

### 4. Download AI Models

Once the containers are running, download an AI model:

```bash
# Download a small, fast model (recommended for testing)
docker exec ollama ollama pull phi3:mini

# Or download a larger, more capable model
docker exec ollama ollama pull llama2:7b

# Verify the model was downloaded
docker exec ollama ollama list
```

### 5. Verify Installation

Check that all services are running:

```bash
# Check container status
docker compose ps

# You should see all containers as "Up"
```

## ðŸŒ Access Points

Once everything is running, you can access:

| Service | URL | Description |
|---------|-----|-------------|
| **First Res-Q App** | http://localhost:3000 | Main chat application |
| **Backend API** | http://localhost:8000 | FastAPI backend |
| **API Documentation** | http://localhost:8000/docs | Interactive API docs |
| **Health Check** | http://localhost:8000/health | System status |
| **Ollama Web UI** | http://localhost:8080 | Model management interface |
| **Qdrant Dashboard** | http://localhost:6333/dashboard | Vector database admin |

## ðŸ’¬ Using the Application

1. **Open the main app** at http://localhost:3000
2. **Choose your AI mode**:
   - Toggle **"Use Local AI"** ON for enhanced RAG + Ollama responses
   - Toggle **"Use Local AI"** OFF for external AnythingLLM
3. **Type a first aid question** like "How do I treat a burn?"
4. **Get intelligent responses** with method indicators:
   - ðŸ¤– **AI + Knowledge Base** - Enhanced responses using both CSV data and LLM reasoning
   - ðŸ¤– **AI Reasoning** - Pure LLM responses when no specific knowledge found
   - ðŸ“š **Knowledge Base Only** - Direct CSV matches when Ollama unavailable
   - ðŸ†˜ **General Advice** - Safe fallback responses

### Example Questions to Try:
- "What should I do for a minor cut?"
- "How do I help someone who is choking?"
- "What are the signs of a heart attack?"
- "How do I treat a sprain?"
- "My child is bleeding"
- "Someone is unconscious"

**Note**: The Local AI will automatically choose the best response method based on question complexity and system availability.

## ðŸ”§ Configuration Options

### Using Different AI Models

```bash
# List available models online
docker exec ollama ollama list

# Pull different models
docker exec ollama ollama pull codellama:7b
docker exec ollama ollama pull mistral:7b
docker exec ollama ollama pull llama2:13b

# Remove models to save space
docker exec ollama ollama rm model_name
```

### Backend Configuration

Edit `backend/main.py` to:
- Change API endpoints
- Modify CORS settings
- Add new routes
- Integrate additional services

### Frontend Customization

Edit files in `frontend/` to:
- Modify the UI (`index.html`, `style.css`)
- Change chat behavior (`chatbot.js`)
- Add new features

## ðŸ›‘ Stopping the Application

```bash
# Stop all services
docker compose down

# Stop and remove all data (WARNING: This deletes downloaded models)
docker compose down --volumes
```

## ðŸ› Troubleshooting

### Common Issues

**1. Port already in use**
```bash
# Check what's using the port
netstat -ano | findstr :3000

# Kill the process or change ports in compose.yaml
```

**2. Out of memory/disk space**
```bash
# Clean up Docker
docker system prune -a

# Remove unused volumes
docker volume prune
```

**3. Models not appearing in Web UI**
```bash
# Restart ollama container
docker compose restart ollama

# Re-download model
docker exec ollama ollama pull phi3:mini
```

**4. Backend can't connect to services**
```bash
# Check if all containers are in the same network
docker network ls
docker network inspect qhelper-ai_appnet
```

### Windows WSL Issues

If you encounter GPU-related errors on Windows:
1. The application will automatically fall back to CPU mode
2. Models will run slower but still function
3. Consider using smaller models like `phi3:mini`

### Performance Optimization

**For faster responses:**
- Use smaller models (`phi3:mini`, `llama2:7b`)
- Allocate more RAM to Docker Desktop
- Use SSD storage for Docker volumes

**For better quality:**
- Use larger models (`llama2:13b`, `llama2:70b`)
- Ensure adequate RAM (16GB+ recommended)

## ðŸ“ Project Structure

```
QHelper-AI/
â”œâ”€â”€ compose.yaml              # Docker services configuration
â”œâ”€â”€ README.md                 # This documentation
â”œâ”€â”€ backend/                  # FastAPI backend
â”‚   â”œâ”€â”€ dockerfile           # Backend container config
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â”œâ”€â”€ main.py             # FastAPI application
â”‚   â”œâ”€â”€ enhanced_rag.py     # Enhanced RAG + Ollama system
â”‚   â”œâ”€â”€ simple_rag.py       # Fallback CSV-only RAG system
â”‚   â”œâ”€â”€ local_rag.py        # Alternative Qdrant-based RAG (unused)
â”‚   â”œâ”€â”€ firstaidqa-*.csv    # First aid Q&A dataset
â”‚   â”œâ”€â”€ getdatabase.py      # Database utilities
â”‚   â””â”€â”€ .env                # Environment variables
â”œâ”€â”€ frontend/               # Static web frontend
â”‚   â”œâ”€â”€ dockerfile         # Frontend container config
â”‚   â”œâ”€â”€ index.html         # Main HTML page
â”‚   â”œâ”€â”€ chatbot.js         # Chat functionality
â”‚   â””â”€â”€ nginx.conf         # Web server config
â””â”€â”€ style.css              # Application styling
```

## ðŸ”„ Development Workflow

### Making Changes

1. **Backend changes**: Edit files in `backend/`, then restart:
   ```bash
   docker compose restart backend
   ```

2. **Frontend changes**: Edit files in `frontend/`, then rebuild:
   ```bash
   docker compose up --build frontend
   ```

3. **Configuration changes**: Edit `compose.yaml`, then:
   ```bash
   docker compose down
   docker compose up --build
   ```

### Adding New Models

```bash
# Research available models
docker exec ollama ollama list --help

# Pull specific model
docker exec ollama ollama pull model_name:tag

# Test model
curl http://localhost:11434/api/generate -d '{
  "model": "model_name",
  "prompt": "Test prompt",
  "stream": false
}'
```

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ðŸ“„ License

[Add your license information here]

## ðŸ†˜ Support

If you encounter issues:

1. Check the [Troubleshooting](#troubleshooting) section
2. Review Docker logs: `docker compose logs service_name`
3. Create an issue with detailed error information
4. Include your system specifications and Docker version

## ðŸ”— Additional Resources

- [Ollama Documentation](https://ollama.ai/docs)
- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Docker Compose Documentation](https://docs.docker.com/compose/)

---

**Happy First Aid Learning! ðŸš‘ðŸ’¡**
